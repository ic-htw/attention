{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from Batch import Batch\n",
    "from data_gen import data_gen\n",
    "from model_seq2seq import Seq2SeqModel\n",
    "from model_generator import Generator\n",
    "from simple_loss_compute import SimpleLossCompute\n",
    "from run_epoch import run_epoch\n",
    "from print_examples import print_examples\n",
    "from iclib.core import init_seed, pt_init, pt_use_cuda, pt_device\n",
    "from iclib.print_info import set_print, toggle_print, p_hb, p_he, p_ti\n",
    "\n",
    "from torchtext import data, datasets\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_perplexity(perplexities):\n",
    "    \"\"\"plot perplexities\"\"\"\n",
    "    plt.title(\"Perplexity per Epoch\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Perplexity\")\n",
    "    plt.plot(perplexities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA: True\n",
      "DEVICE: cuda:0\n"
     ]
    }
   ],
   "source": [
    "pt_init()\n",
    "#pt_init(use_gpu=False)\n",
    "print(\"CUDA:\", pt_use_cuda())\n",
    "print(\"DEVICE:\", pt_device())\n",
    "\n",
    "init_seed(42)\n",
    "set_print(False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy_de = spacy.load('de')\n",
    "spacy_en = spacy.load('en')\n",
    "\n",
    "def tokenize_de(text):\n",
    "    return [tok.text for tok in spacy_de.tokenizer(text)]\n",
    "\n",
    "def tokenize_en(text):\n",
    "    return [tok.text for tok in spacy_en.tokenizer(text)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_data_info(train_data, valid_data, test_data, src_field, trg_field):\n",
    "    \"\"\" This prints some useful stuff about our data sets. \"\"\"\n",
    "\n",
    "    print(\"Data set sizes (number of sentence pairs):\")\n",
    "    print('train', len(train_data))\n",
    "    print('valid', len(valid_data))\n",
    "    print('test', len(test_data), \"\\n\")\n",
    "\n",
    "    print(\"First training example:\")\n",
    "    print(\"src:\", \" \".join(vars(train_data[0])['src']))\n",
    "    print(\"trg:\", \" \".join(vars(train_data[0])['trg']), \"\\n\")\n",
    "\n",
    "    print(\"Most common words (src):\")\n",
    "    print(\"\\n\".join([\"%10s %10d\" % x for x in src_field.vocab.freqs.most_common(10)]), \"\\n\")\n",
    "    print(\"Most common words (trg):\")\n",
    "    print(\"\\n\".join([\"%10s %10d\" % x for x in trg_field.vocab.freqs.most_common(10)]), \"\\n\")\n",
    "\n",
    "    print(\"First 10 words (src):\")\n",
    "    print(\"\\n\".join(\n",
    "        '%02d %s' % (i, t) for i, t in enumerate(src_field.vocab.itos[:10])), \"\\n\")\n",
    "    print(\"First 10 words (trg):\")\n",
    "    print(\"\\n\".join(\n",
    "        '%02d %s' % (i, t) for i, t in enumerate(trg_field.vocab.itos[:10])), \"\\n\")\n",
    "\n",
    "    print(\"Number of German words (types):\", len(src_field.vocab))\n",
    "    print(\"Number of English words (types):\", len(trg_field.vocab), \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rebatch(pad_idx, batch):\n",
    "    \"\"\"Wrap torchtext batch into our own Batch class for pre-processing\"\"\"\n",
    "    return Batch(batch.src, batch.trg, pad_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "UNK_TOKEN = \"<unk>\"\n",
    "PAD_TOKEN = \"<pad>\"    \n",
    "SOS_TOKEN = \"<s>\"\n",
    "EOS_TOKEN = \"</s>\"\n",
    "LOWER = True\n",
    "\n",
    "# we include lengths to provide to the RNNs\n",
    "SRC = data.Field(tokenize=tokenize_de, \n",
    "                 batch_first=True, lower=LOWER, include_lengths=True,\n",
    "                 unk_token=UNK_TOKEN, pad_token=PAD_TOKEN, init_token=None, eos_token=EOS_TOKEN)\n",
    "TRG = data.Field(tokenize=tokenize_en, \n",
    "                 batch_first=True, lower=LOWER, include_lengths=True,\n",
    "                 unk_token=UNK_TOKEN, pad_token=PAD_TOKEN, init_token=SOS_TOKEN, eos_token=EOS_TOKEN)\n",
    "\n",
    "MAX_LEN = 25  # NOTE: we filter out a lot of sentences for speed\n",
    "train_data, valid_data, test_data = datasets.IWSLT.splits(\n",
    "    exts=('.de', '.en'), fields=(SRC, TRG), \n",
    "    filter_pred=lambda x: len(vars(x)['src']) <= MAX_LEN and len(vars(x)['trg']) <= MAX_LEN)\n",
    "MIN_FREQ = 5  # NOTE: we limit the vocabulary to frequent words for speed\n",
    "SRC.build_vocab(train_data.src, min_freq=MIN_FREQ)\n",
    "TRG.build_vocab(train_data.trg, min_freq=MIN_FREQ)\n",
    "    \n",
    "PAD_INDEX = TRG.vocab.stoi[PAD_TOKEN]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data set sizes (number of sentence pairs):\n",
      "train 143116\n",
      "valid 690\n",
      "test 963 \n",
      "\n",
      "First training example:\n",
      "src: david gallo : das ist bill lange . ich bin dave gallo .\n",
      "trg: david gallo : this is bill lange . i 'm dave gallo . \n",
      "\n",
      "Most common words (src):\n",
      "         .     138325\n",
      "         ,     105944\n",
      "       und      41839\n",
      "       die      40809\n",
      "       das      33324\n",
      "       sie      33035\n",
      "       ich      31153\n",
      "       ist      31035\n",
      "        es      27449\n",
      "       wir      25817 \n",
      "\n",
      "Most common words (trg):\n",
      "         .     137259\n",
      "         ,      91619\n",
      "       the      73344\n",
      "       and      50273\n",
      "        to      42798\n",
      "         a      39573\n",
      "        of      39496\n",
      "         i      33524\n",
      "        it      32921\n",
      "      that      32643 \n",
      "\n",
      "First 10 words (src):\n",
      "00 <unk>\n",
      "01 <pad>\n",
      "02 </s>\n",
      "03 .\n",
      "04 ,\n",
      "05 und\n",
      "06 die\n",
      "07 das\n",
      "08 sie\n",
      "09 ich \n",
      "\n",
      "First 10 words (trg):\n",
      "00 <unk>\n",
      "01 <pad>\n",
      "02 <s>\n",
      "03 </s>\n",
      "04 .\n",
      "05 ,\n",
      "06 the\n",
      "07 and\n",
      "08 to\n",
      "09 a \n",
      "\n",
      "Number of German words (types): 15761\n",
      "Number of English words (types): 13003 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_data_info(train_data, valid_data, test_data, SRC, TRG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iter = data.BucketIterator(train_data, batch_size=64, train=True, \n",
    "                                 sort_within_batch=True, \n",
    "                                 sort_key=lambda x: (len(x.src), len(x.trg)), repeat=False,\n",
    "                                 device=pt_device())\n",
    "valid_iter = data.Iterator(valid_data, batch_size=1, train=False, sort=False, repeat=False, \n",
    "                           device=pt_device())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "Vs = len(SRC.vocab)\n",
    "Vt = len(TRG.vocab)\n",
    "E = 256\n",
    "H = 256\n",
    "L = 1\n",
    "dropout=0.2\n",
    "\n",
    "lr=0.0003\n",
    "print_every=100\n",
    "\n",
    "model = Seq2SeqModel(Vs, Vt, E, E, H, L, dropout, Generator(H, Vt))\n",
    "if pt_use_cuda():\n",
    "    model.cuda(pt_device())\n",
    "    \n",
    "criterion = nn.NLLLoss(reduction=\"sum\", ignore_index=PAD_INDEX)\n",
    "optim = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "dev_perplexities = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n",
      "Epoch Step: 100 Loss: 14.555441 Tokens per Sec: 26400.116053\n",
      "Epoch Step: 200 Loss: 101.337227 Tokens per Sec: 26429.283250\n",
      "Epoch Step: 300 Loss: 25.553289 Tokens per Sec: 26846.771013\n",
      "Epoch Step: 400 Loss: 85.731422 Tokens per Sec: 26801.431006\n",
      "Epoch Step: 500 Loss: 40.868401 Tokens per Sec: 26562.451268\n",
      "Epoch Step: 600 Loss: 38.950787 Tokens per Sec: 26703.626247\n",
      "Epoch Step: 700 Loss: 101.915741 Tokens per Sec: 26775.444644\n",
      "Epoch Step: 800 Loss: 105.231644 Tokens per Sec: 26747.190714\n",
      "Epoch Step: 900 Loss: 33.671410 Tokens per Sec: 26668.076857\n",
      "Epoch Step: 1000 Loss: 75.869713 Tokens per Sec: 26255.632681\n",
      "Epoch Step: 1100 Loss: 109.312073 Tokens per Sec: 26517.122609\n",
      "Epoch Step: 1200 Loss: 76.700005 Tokens per Sec: 27030.055759\n",
      "Epoch Step: 1300 Loss: 71.037125 Tokens per Sec: 27091.202376\n",
      "Epoch Step: 1400 Loss: 28.032452 Tokens per Sec: 26432.123295\n",
      "Epoch Step: 1500 Loss: 35.978996 Tokens per Sec: 25909.152480\n",
      "Epoch Step: 1600 Loss: 59.478058 Tokens per Sec: 26748.856571\n",
      "Epoch Step: 1700 Loss: 56.866684 Tokens per Sec: 26775.135762\n",
      "Epoch Step: 1800 Loss: 35.285217 Tokens per Sec: 26887.182281\n",
      "Epoch Step: 1900 Loss: 84.892250 Tokens per Sec: 26952.403271\n",
      "Epoch Step: 2000 Loss: 52.656868 Tokens per Sec: 26997.200890\n",
      "Epoch Step: 2100 Loss: 51.673500 Tokens per Sec: 27149.674727\n",
      "Epoch Step: 2200 Loss: 20.923550 Tokens per Sec: 26584.288017\n",
      "\n",
      "Example #1\n",
      "Src :  als ich 11 jahre alt war , wurde ich eines morgens von den <unk> heller freude geweckt .\n",
      "Trg :  when i was 11 , i remember waking up one morning to the sound of joy in my house .\n",
      "Pred:  when i was 15 years old , i was a couple of a <unk> of the <unk> .\n",
      "\n",
      "Example #2\n",
      "Src :  mein vater hörte sich auf seinem kleinen , grauen radio die <unk> der bbc an .\n",
      "Trg :  my father was listening to bbc news on his small , gray radio .\n",
      "Pred:  my father was on the <unk> , the <unk> of the <unk> .\n",
      "\n",
      "Example #3\n",
      "Src :  er sah sehr glücklich aus , was damals ziemlich ungewöhnlich war , da ihn die nachrichten meistens <unk> .\n",
      "Trg :  there was a big smile on his face which was unusual then , because the news mostly depressed him .\n",
      "Pred:  he was very interested in the same thing , what was the great thing was , because it was the <unk> .\n",
      "\n",
      "Validation perplexity: 32.189866\n",
      "Epoch 1\n",
      "Epoch Step: 100 Loss: 80.382996 Tokens per Sec: 25932.185246\n",
      "Epoch Step: 200 Loss: 54.671532 Tokens per Sec: 26777.710286\n",
      "Epoch Step: 300 Loss: 18.874252 Tokens per Sec: 26867.980082\n",
      "Epoch Step: 400 Loss: 54.243267 Tokens per Sec: 27066.633823\n",
      "Epoch Step: 500 Loss: 32.769855 Tokens per Sec: 26869.504390\n",
      "Epoch Step: 600 Loss: 4.651629 Tokens per Sec: 27133.688635\n",
      "Epoch Step: 700 Loss: 67.567444 Tokens per Sec: 26877.871279\n",
      "Epoch Step: 800 Loss: 44.373344 Tokens per Sec: 26959.810947\n",
      "Epoch Step: 900 Loss: 19.289629 Tokens per Sec: 26775.555965\n",
      "Epoch Step: 1000 Loss: 63.261730 Tokens per Sec: 27168.114347\n",
      "Epoch Step: 1100 Loss: 29.133894 Tokens per Sec: 26917.707112\n",
      "Epoch Step: 1200 Loss: 37.077591 Tokens per Sec: 26881.214771\n",
      "Epoch Step: 1300 Loss: 52.298271 Tokens per Sec: 26620.083699\n",
      "Epoch Step: 1400 Loss: 61.906261 Tokens per Sec: 26477.676780\n",
      "Epoch Step: 1500 Loss: 45.937370 Tokens per Sec: 26564.004036\n",
      "Epoch Step: 1600 Loss: 55.690636 Tokens per Sec: 26906.828644\n",
      "Epoch Step: 1700 Loss: 83.699020 Tokens per Sec: 26800.184466\n",
      "Epoch Step: 1800 Loss: 41.715042 Tokens per Sec: 26635.836027\n",
      "Epoch Step: 1900 Loss: 85.560570 Tokens per Sec: 26634.840084\n",
      "Epoch Step: 2000 Loss: 73.766045 Tokens per Sec: 26453.608915\n",
      "Epoch Step: 2100 Loss: 41.885319 Tokens per Sec: 26435.500321\n",
      "Epoch Step: 2200 Loss: 14.902601 Tokens per Sec: 27194.523010\n",
      "\n",
      "Example #1\n",
      "Src :  als ich 11 jahre alt war , wurde ich eines morgens von den <unk> heller freude geweckt .\n",
      "Trg :  when i was 11 , i remember waking up one morning to the sound of joy in my house .\n",
      "Pred:  when i was 11 years old , i was a few of a <unk> of the <unk> .\n",
      "\n",
      "Example #2\n",
      "Src :  mein vater hörte sich auf seinem kleinen , grauen radio die <unk> der bbc an .\n",
      "Trg :  my father was listening to bbc news on his small , gray radio .\n",
      "Pred:  my father had to be on the little <unk> , the <unk> of the <unk> .\n",
      "\n",
      "Example #3\n",
      "Src :  er sah sehr glücklich aus , was damals ziemlich ungewöhnlich war , da ihn die nachrichten meistens <unk> .\n",
      "Trg :  there was a big smile on his face which was unusual then , because the news mostly depressed him .\n",
      "Pred:  he saw the very happy , which was a very long time , and he was the <unk> .\n",
      "\n",
      "Validation perplexity: 20.680522\n",
      "Epoch 2\n",
      "Epoch Step: 100 Loss: 33.900761 Tokens per Sec: 25336.335706\n",
      "Epoch Step: 200 Loss: 29.913054 Tokens per Sec: 26828.890036\n",
      "Epoch Step: 300 Loss: 60.804668 Tokens per Sec: 26679.819914\n",
      "Epoch Step: 400 Loss: 51.872849 Tokens per Sec: 26862.071806\n",
      "Epoch Step: 500 Loss: 49.693237 Tokens per Sec: 26814.293228\n",
      "Epoch Step: 600 Loss: 34.792931 Tokens per Sec: 26756.799388\n",
      "Epoch Step: 700 Loss: 51.485180 Tokens per Sec: 26629.394374\n",
      "Epoch Step: 800 Loss: 71.767044 Tokens per Sec: 26865.561272\n",
      "Epoch Step: 900 Loss: 42.468483 Tokens per Sec: 26580.300291\n",
      "Epoch Step: 1000 Loss: 30.023098 Tokens per Sec: 26804.781051\n",
      "Epoch Step: 1100 Loss: 59.084126 Tokens per Sec: 26653.270038\n",
      "Epoch Step: 1200 Loss: 15.621619 Tokens per Sec: 26425.987138\n",
      "Epoch Step: 1300 Loss: 44.998611 Tokens per Sec: 26854.244692\n",
      "Epoch Step: 1400 Loss: 52.849995 Tokens per Sec: 27149.988499\n",
      "Epoch Step: 1500 Loss: 62.408306 Tokens per Sec: 26482.455673\n",
      "Epoch Step: 1600 Loss: 34.333412 Tokens per Sec: 26651.854610\n",
      "Epoch Step: 1700 Loss: 72.280510 Tokens per Sec: 27067.281730\n",
      "Epoch Step: 1800 Loss: 49.376503 Tokens per Sec: 26694.048980\n",
      "Epoch Step: 1900 Loss: 20.776289 Tokens per Sec: 27132.141985\n",
      "Epoch Step: 2000 Loss: 39.052143 Tokens per Sec: 26825.563812\n",
      "Epoch Step: 2100 Loss: 59.757496 Tokens per Sec: 26894.990205\n",
      "Epoch Step: 2200 Loss: 29.135540 Tokens per Sec: 26872.402878\n",
      "\n",
      "Example #1\n",
      "Src :  als ich 11 jahre alt war , wurde ich eines morgens von den <unk> heller freude geweckt .\n",
      "Trg :  when i was 11 , i remember waking up one morning to the sound of joy in my house .\n",
      "Pred:  when i was 11 years old , i was a <unk> of the <unk> of the <unk> .\n",
      "\n",
      "Example #2\n",
      "Src :  mein vater hörte sich auf seinem kleinen , grauen radio die <unk> der bbc an .\n",
      "Trg :  my father was listening to bbc news on his small , gray radio .\n",
      "Pred:  my father had on the small , the <unk> of the bbc .\n",
      "\n",
      "Example #3\n",
      "Src :  er sah sehr glücklich aus , was damals ziemlich ungewöhnlich war , da ihn die nachrichten meistens <unk> .\n",
      "Trg :  there was a big smile on his face which was unusual then , because the news mostly depressed him .\n",
      "Pred:  he saw very happy , what was pretty much , and he was the news to the news .\n",
      "\n",
      "Validation perplexity: 15.490052\n",
      "Epoch 3\n",
      "Epoch Step: 100 Loss: 26.940006 Tokens per Sec: 25397.816358\n",
      "Epoch Step: 200 Loss: 60.737339 Tokens per Sec: 26535.919829\n",
      "Epoch Step: 300 Loss: 62.586609 Tokens per Sec: 26460.359111\n",
      "Epoch Step: 400 Loss: 59.144459 Tokens per Sec: 27754.050014\n",
      "Epoch Step: 500 Loss: 15.359282 Tokens per Sec: 28498.078312\n",
      "Epoch Step: 600 Loss: 50.368591 Tokens per Sec: 28104.817214\n",
      "Epoch Step: 700 Loss: 24.767460 Tokens per Sec: 26976.954179\n",
      "Epoch Step: 800 Loss: 18.757904 Tokens per Sec: 26969.655653\n",
      "Epoch Step: 900 Loss: 18.787302 Tokens per Sec: 26981.008768\n",
      "Epoch Step: 1000 Loss: 60.792019 Tokens per Sec: 26789.894916\n",
      "Epoch Step: 1100 Loss: 24.323246 Tokens per Sec: 26931.222990\n",
      "Epoch Step: 1200 Loss: 15.388385 Tokens per Sec: 26804.797543\n",
      "Epoch Step: 1300 Loss: 71.623604 Tokens per Sec: 26835.304534\n",
      "Epoch Step: 1400 Loss: 26.514563 Tokens per Sec: 26818.773350\n",
      "Epoch Step: 1500 Loss: 42.485538 Tokens per Sec: 26595.046469\n",
      "Epoch Step: 1600 Loss: 25.370281 Tokens per Sec: 26412.282994\n",
      "Epoch Step: 1700 Loss: 23.192305 Tokens per Sec: 26776.057319\n",
      "Epoch Step: 1800 Loss: 31.596395 Tokens per Sec: 26724.608951\n",
      "Epoch Step: 1900 Loss: 38.075916 Tokens per Sec: 26985.879282\n",
      "Epoch Step: 2000 Loss: 26.393801 Tokens per Sec: 26774.604822\n",
      "Epoch Step: 2100 Loss: 50.218941 Tokens per Sec: 26807.234297\n",
      "Epoch Step: 2200 Loss: 29.856148 Tokens per Sec: 26817.739280\n",
      "\n",
      "Example #1\n",
      "Src :  als ich 11 jahre alt war , wurde ich eines morgens von den <unk> heller freude geweckt .\n",
      "Trg :  when i was 11 , i remember waking up one morning to the sound of joy in my house .\n",
      "Pred:  when i was 11 years old , i was a <unk> of the <unk> of joy .\n",
      "\n",
      "Example #2\n",
      "Src :  mein vater hörte sich auf seinem kleinen , grauen radio die <unk> der bbc an .\n",
      "Trg :  my father was listening to bbc news on his small , gray radio .\n",
      "Pred:  my father had to <unk> on his small , the <unk> of the bbc of the bbc .\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example #3\n",
      "Src :  er sah sehr glücklich aus , was damals ziemlich ungewöhnlich war , da ihn die nachrichten meistens <unk> .\n",
      "Trg :  there was a big smile on his face which was unusual then , because the news mostly depressed him .\n",
      "Pred:  he looked very happy , what was pretty much , and he was the <unk> to <unk> the news .\n",
      "\n",
      "Validation perplexity: 13.930233\n",
      "Epoch 4\n",
      "Epoch Step: 100 Loss: 43.464630 Tokens per Sec: 25706.334840\n",
      "Epoch Step: 200 Loss: 33.499287 Tokens per Sec: 26835.350015\n",
      "Epoch Step: 300 Loss: 59.188469 Tokens per Sec: 26950.677089\n",
      "Epoch Step: 400 Loss: 61.002441 Tokens per Sec: 26906.814062\n",
      "Epoch Step: 500 Loss: 35.533924 Tokens per Sec: 26742.721270\n",
      "Epoch Step: 600 Loss: 12.945827 Tokens per Sec: 26789.609596\n",
      "Epoch Step: 700 Loss: 25.883720 Tokens per Sec: 26967.805200\n",
      "Epoch Step: 800 Loss: 38.157066 Tokens per Sec: 26854.720429\n",
      "Epoch Step: 900 Loss: 14.314346 Tokens per Sec: 26647.657480\n",
      "Epoch Step: 1000 Loss: 37.209507 Tokens per Sec: 26615.144060\n",
      "Epoch Step: 1100 Loss: 21.906662 Tokens per Sec: 26828.869013\n",
      "Epoch Step: 1200 Loss: 50.460144 Tokens per Sec: 26777.473091\n",
      "Epoch Step: 1300 Loss: 3.112169 Tokens per Sec: 26834.452224\n",
      "Epoch Step: 1400 Loss: 7.653388 Tokens per Sec: 26984.548656\n",
      "Epoch Step: 1500 Loss: 64.469475 Tokens per Sec: 27016.488402\n",
      "Epoch Step: 1600 Loss: 63.055332 Tokens per Sec: 26995.581817\n",
      "Epoch Step: 1700 Loss: 37.151371 Tokens per Sec: 26699.764314\n",
      "Epoch Step: 1800 Loss: 57.201843 Tokens per Sec: 26602.576812\n",
      "Epoch Step: 1900 Loss: 18.464918 Tokens per Sec: 27282.531187\n",
      "Epoch Step: 2000 Loss: 19.682350 Tokens per Sec: 26815.110084\n",
      "Epoch Step: 2100 Loss: 32.437004 Tokens per Sec: 26843.831418\n",
      "Epoch Step: 2200 Loss: 26.064007 Tokens per Sec: 26750.448633\n",
      "\n",
      "Example #1\n",
      "Src :  als ich 11 jahre alt war , wurde ich eines morgens von den <unk> heller freude geweckt .\n",
      "Trg :  when i was 11 , i remember waking up one morning to the sound of joy in my house .\n",
      "Pred:  when i was 11 years old , i was a number of <unk> of the <unk> of joy .\n",
      "\n",
      "Example #2\n",
      "Src :  mein vater hörte sich auf seinem kleinen , grauen radio die <unk> der bbc an .\n",
      "Trg :  my father was listening to bbc news on his small , gray radio .\n",
      "Pred:  my father had on his little <unk> , radio radio radio <unk> the <unk> .\n",
      "\n",
      "Example #3\n",
      "Src :  er sah sehr glücklich aus , was damals ziemlich ungewöhnlich war , da ihn die nachrichten meistens <unk> .\n",
      "Trg :  there was a big smile on his face which was unusual then , because the news mostly depressed him .\n",
      "Pred:  he looked very happy , what was pretty unusual , because it was the most popular <unk> .\n",
      "\n",
      "Validation perplexity: 12.799932\n",
      "Epoch 5\n",
      "Epoch Step: 100 Loss: 42.858028 Tokens per Sec: 25571.210406\n",
      "Epoch Step: 200 Loss: 49.594963 Tokens per Sec: 26781.038821\n",
      "Epoch Step: 300 Loss: 54.684475 Tokens per Sec: 27161.348817\n",
      "Epoch Step: 400 Loss: 25.984240 Tokens per Sec: 26721.767713\n",
      "Epoch Step: 500 Loss: 10.269564 Tokens per Sec: 26858.172260\n",
      "Epoch Step: 600 Loss: 58.475002 Tokens per Sec: 26902.993488\n",
      "Epoch Step: 700 Loss: 23.392065 Tokens per Sec: 26108.689874\n",
      "Epoch Step: 800 Loss: 35.322361 Tokens per Sec: 26787.702315\n",
      "Epoch Step: 900 Loss: 14.360238 Tokens per Sec: 27259.716702\n",
      "Epoch Step: 1000 Loss: 22.749819 Tokens per Sec: 26683.360247\n",
      "Epoch Step: 1100 Loss: 54.047707 Tokens per Sec: 26673.600755\n",
      "Epoch Step: 1200 Loss: 19.951757 Tokens per Sec: 27225.653123\n",
      "Epoch Step: 1300 Loss: 14.409762 Tokens per Sec: 26637.956834\n",
      "Epoch Step: 1400 Loss: 51.347324 Tokens per Sec: 26562.113162\n",
      "Epoch Step: 1500 Loss: 7.534847 Tokens per Sec: 26652.696515\n",
      "Epoch Step: 1600 Loss: 22.382765 Tokens per Sec: 26818.686010\n",
      "Epoch Step: 1700 Loss: 35.023445 Tokens per Sec: 30330.183954\n",
      "Epoch Step: 1800 Loss: 56.381557 Tokens per Sec: 28281.597132\n",
      "Epoch Step: 1900 Loss: 58.881447 Tokens per Sec: 26983.349265\n",
      "Epoch Step: 2000 Loss: 44.934341 Tokens per Sec: 26706.997300\n",
      "Epoch Step: 2100 Loss: 35.877468 Tokens per Sec: 26800.700141\n",
      "Epoch Step: 2200 Loss: 57.766159 Tokens per Sec: 27232.789720\n",
      "\n",
      "Example #1\n",
      "Src :  als ich 11 jahre alt war , wurde ich eines morgens von den <unk> heller freude geweckt .\n",
      "Trg :  when i was 11 , i remember waking up one morning to the sound of joy in my house .\n",
      "Pred:  when i was 11 years old , i was a <unk> of the <unk> of joy .\n",
      "\n",
      "Example #2\n",
      "Src :  mein vater hörte sich auf seinem kleinen , grauen radio die <unk> der bbc an .\n",
      "Trg :  my father was listening to bbc news on his small , gray radio .\n",
      "Pred:  my dad had to <unk> on his little bit , radio radio <unk> the <unk> of the bbc .\n",
      "\n",
      "Example #3\n",
      "Src :  er sah sehr glücklich aus , was damals ziemlich ungewöhnlich war , da ihn die nachrichten meistens <unk> .\n",
      "Trg :  there was a big smile on his face which was unusual then , because the news mostly depressed him .\n",
      "Pred:  he looked very happy , what was unusual at the time , and it was the most important to <unk> .\n",
      "\n",
      "Validation perplexity: 12.197019\n",
      "Epoch 6\n",
      "Epoch Step: 100 Loss: 12.093596 Tokens per Sec: 25600.550545\n",
      "Epoch Step: 200 Loss: 15.253960 Tokens per Sec: 25973.830714\n",
      "Epoch Step: 300 Loss: 19.335691 Tokens per Sec: 26227.191940\n",
      "Epoch Step: 400 Loss: 10.579150 Tokens per Sec: 26490.429172\n",
      "Epoch Step: 500 Loss: 31.701572 Tokens per Sec: 27724.209100\n",
      "Epoch Step: 600 Loss: 44.203022 Tokens per Sec: 26460.598551\n",
      "Epoch Step: 700 Loss: 10.455235 Tokens per Sec: 26957.614695\n",
      "Epoch Step: 800 Loss: 28.714190 Tokens per Sec: 26694.003033\n",
      "Epoch Step: 900 Loss: 49.764549 Tokens per Sec: 27047.904676\n",
      "Epoch Step: 1000 Loss: 15.431935 Tokens per Sec: 26799.417145\n",
      "Epoch Step: 1100 Loss: 19.953510 Tokens per Sec: 26868.106135\n",
      "Epoch Step: 1200 Loss: 17.736538 Tokens per Sec: 26857.244223\n",
      "Epoch Step: 1300 Loss: 57.870213 Tokens per Sec: 27155.846806\n",
      "Epoch Step: 1400 Loss: 46.150158 Tokens per Sec: 26382.304230\n",
      "Epoch Step: 1500 Loss: 30.628832 Tokens per Sec: 27035.444198\n",
      "Epoch Step: 1600 Loss: 22.081287 Tokens per Sec: 26945.647399\n",
      "Epoch Step: 1700 Loss: 16.083513 Tokens per Sec: 26857.090895\n",
      "Epoch Step: 1800 Loss: 24.491413 Tokens per Sec: 26545.242238\n",
      "Epoch Step: 1900 Loss: 40.989567 Tokens per Sec: 26637.540813\n",
      "Epoch Step: 2000 Loss: 55.178944 Tokens per Sec: 26831.095291\n",
      "Epoch Step: 2100 Loss: 39.872669 Tokens per Sec: 27007.429082\n",
      "Epoch Step: 2200 Loss: 30.270403 Tokens per Sec: 27273.740203\n",
      "\n",
      "Example #1\n",
      "Src :  als ich 11 jahre alt war , wurde ich eines morgens von den <unk> heller freude geweckt .\n",
      "Trg :  when i was 11 , i remember waking up one morning to the sound of joy in my house .\n",
      "Pred:  when i was 11 years old , i was able to be a <unk> of the <unk> of joy .\n",
      "\n",
      "Example #2\n",
      "Src :  mein vater hörte sich auf seinem kleinen , grauen radio die <unk> der bbc an .\n",
      "Trg :  my father was listening to bbc news on his small , gray radio .\n",
      "Pred:  my dad was on his little , little <unk> , the <unk> of the bbc .\n",
      "\n",
      "Example #3\n",
      "Src :  er sah sehr glücklich aus , was damals ziemlich ungewöhnlich war , da ihn die nachrichten meistens <unk> .\n",
      "Trg :  there was a big smile on his face which was unusual then , because the news mostly depressed him .\n",
      "Pred:  he saw a very happy thing , which was pretty unusual , because it was the most popular <unk> .\n",
      "\n",
      "Validation perplexity: 11.977105\n",
      "Epoch 7\n",
      "Epoch Step: 100 Loss: 24.381128 Tokens per Sec: 25446.425601\n",
      "Epoch Step: 200 Loss: 47.012009 Tokens per Sec: 26797.356847\n",
      "Epoch Step: 300 Loss: 25.037460 Tokens per Sec: 26939.577075\n",
      "Epoch Step: 400 Loss: 50.720905 Tokens per Sec: 26969.596237\n",
      "Epoch Step: 500 Loss: 32.097794 Tokens per Sec: 26764.174882\n",
      "Epoch Step: 600 Loss: 40.542988 Tokens per Sec: 26316.964248\n",
      "Epoch Step: 700 Loss: 13.125290 Tokens per Sec: 26400.244962\n",
      "Epoch Step: 800 Loss: 17.224794 Tokens per Sec: 26484.078580\n",
      "Epoch Step: 900 Loss: 31.952007 Tokens per Sec: 31427.205843\n",
      "Epoch Step: 1000 Loss: 42.030090 Tokens per Sec: 26705.166334\n",
      "Epoch Step: 1100 Loss: 41.634842 Tokens per Sec: 26794.213023\n",
      "Epoch Step: 1200 Loss: 30.817663 Tokens per Sec: 26612.967416\n",
      "Epoch Step: 1300 Loss: 27.376799 Tokens per Sec: 27001.161072\n",
      "Epoch Step: 1400 Loss: 9.562539 Tokens per Sec: 27004.549365\n",
      "Epoch Step: 1500 Loss: 17.888483 Tokens per Sec: 26823.198244\n",
      "Epoch Step: 1600 Loss: 31.676901 Tokens per Sec: 27020.393107\n",
      "Epoch Step: 1700 Loss: 17.617805 Tokens per Sec: 27342.869470\n",
      "Epoch Step: 1800 Loss: 53.711021 Tokens per Sec: 26150.750081\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch Step: 1900 Loss: 40.171597 Tokens per Sec: 26299.454028\n",
      "Epoch Step: 2000 Loss: 54.483505 Tokens per Sec: 26954.603110\n",
      "Epoch Step: 2100 Loss: 39.235950 Tokens per Sec: 27493.054904\n",
      "Epoch Step: 2200 Loss: 21.875202 Tokens per Sec: 26653.546668\n",
      "\n",
      "Example #1\n",
      "Src :  als ich 11 jahre alt war , wurde ich eines morgens von den <unk> heller freude geweckt .\n",
      "Trg :  when i was 11 , i remember waking up one morning to the sound of joy in my house .\n",
      "Pred:  when i was 11 years old , i was one of the <unk> of the joy of joy .\n",
      "\n",
      "Example #2\n",
      "Src :  mein vater hörte sich auf seinem kleinen , grauen radio die <unk> der bbc an .\n",
      "Trg :  my father was listening to bbc news on his small , gray radio .\n",
      "Pred:  my father had focused on his little , <unk> radio shack the <unk> of the bbc .\n",
      "\n",
      "Example #3\n",
      "Src :  er sah sehr glücklich aus , was damals ziemlich ungewöhnlich war , da ihn die nachrichten meistens <unk> .\n",
      "Trg :  there was a big smile on his face which was unusual then , because the news mostly depressed him .\n",
      "Pred:  he looked very happy , what was unusual at the time , because it was the most <unk> most <unk> .\n",
      "\n",
      "Validation perplexity: 11.828500\n",
      "Epoch 8\n",
      "Epoch Step: 100 Loss: 28.215302 Tokens per Sec: 25430.476329\n",
      "Epoch Step: 200 Loss: 33.044483 Tokens per Sec: 26734.530379\n",
      "Epoch Step: 300 Loss: 41.286156 Tokens per Sec: 26981.743464\n",
      "Epoch Step: 400 Loss: 35.037861 Tokens per Sec: 26764.611800\n",
      "Epoch Step: 500 Loss: 38.838425 Tokens per Sec: 26946.754497\n",
      "Epoch Step: 600 Loss: 46.467327 Tokens per Sec: 27200.890039\n",
      "Epoch Step: 700 Loss: 53.368725 Tokens per Sec: 26877.346080\n",
      "Epoch Step: 800 Loss: 19.357468 Tokens per Sec: 27093.901667\n",
      "Epoch Step: 900 Loss: 38.248665 Tokens per Sec: 26752.979452\n",
      "Epoch Step: 1000 Loss: 31.739923 Tokens per Sec: 26631.371197\n",
      "Epoch Step: 1100 Loss: 16.571573 Tokens per Sec: 26923.814368\n",
      "Epoch Step: 1200 Loss: 31.773809 Tokens per Sec: 26988.593215\n",
      "Epoch Step: 1300 Loss: 36.207932 Tokens per Sec: 26826.464613\n",
      "Epoch Step: 1400 Loss: 40.832420 Tokens per Sec: 26799.746819\n",
      "Epoch Step: 1500 Loss: 39.163548 Tokens per Sec: 27046.675283\n",
      "Epoch Step: 1600 Loss: 24.794807 Tokens per Sec: 26782.493451\n",
      "Epoch Step: 1700 Loss: 32.746822 Tokens per Sec: 26738.133917\n",
      "Epoch Step: 1800 Loss: 8.678872 Tokens per Sec: 26545.509927\n",
      "Epoch Step: 1900 Loss: 39.990158 Tokens per Sec: 26862.160634\n",
      "Epoch Step: 2000 Loss: 22.446289 Tokens per Sec: 26955.975030\n",
      "Epoch Step: 2100 Loss: 51.860130 Tokens per Sec: 26911.995000\n",
      "Epoch Step: 2200 Loss: 29.945276 Tokens per Sec: 26257.368751\n",
      "\n",
      "Example #1\n",
      "Src :  als ich 11 jahre alt war , wurde ich eines morgens von den <unk> heller freude geweckt .\n",
      "Trg :  when i was 11 , i remember waking up one morning to the sound of joy in my house .\n",
      "Pred:  when i was 11 years old , i was one of the <unk> of joy pleasure .\n",
      "\n",
      "Example #2\n",
      "Src :  mein vater hörte sich auf seinem kleinen , grauen radio die <unk> der bbc an .\n",
      "Trg :  my father was listening to bbc news on his small , gray radio .\n",
      "Pred:  my father had to <unk> on his little <unk> , radio <unk> the <unk> of the bbc .\n",
      "\n",
      "Example #3\n",
      "Src :  er sah sehr glücklich aus , was damals ziemlich ungewöhnlich war , da ihn die nachrichten meistens <unk> .\n",
      "Trg :  there was a big smile on his face which was unusual then , because the news mostly depressed him .\n",
      "Pred:  he looked very happy , which was pretty unusual , and it was the most <unk> to <unk> .\n",
      "\n",
      "Validation perplexity: 11.895033\n",
      "Epoch 9\n",
      "Epoch Step: 100 Loss: 15.423994 Tokens per Sec: 25459.920729\n",
      "Epoch Step: 200 Loss: 32.253403 Tokens per Sec: 26627.421962\n",
      "Epoch Step: 300 Loss: 31.835691 Tokens per Sec: 26301.735449\n",
      "Epoch Step: 400 Loss: 24.489195 Tokens per Sec: 26698.973455\n",
      "Epoch Step: 500 Loss: 30.429783 Tokens per Sec: 26592.548135\n",
      "Epoch Step: 600 Loss: 21.612566 Tokens per Sec: 26677.944171\n",
      "Epoch Step: 700 Loss: 21.559038 Tokens per Sec: 27002.298974\n",
      "Epoch Step: 800 Loss: 40.070686 Tokens per Sec: 26698.458759\n",
      "Epoch Step: 900 Loss: 45.263004 Tokens per Sec: 26776.031017\n",
      "Epoch Step: 1000 Loss: 20.402971 Tokens per Sec: 26272.301760\n",
      "Epoch Step: 1100 Loss: 36.315689 Tokens per Sec: 26784.622857\n",
      "Epoch Step: 1200 Loss: 43.237301 Tokens per Sec: 26651.114623\n",
      "Epoch Step: 1300 Loss: 30.605844 Tokens per Sec: 26883.359137\n",
      "Epoch Step: 1400 Loss: 35.450722 Tokens per Sec: 26783.225804\n",
      "Epoch Step: 1500 Loss: 24.439802 Tokens per Sec: 26694.849510\n",
      "Epoch Step: 1600 Loss: 23.068262 Tokens per Sec: 26723.023183\n",
      "Epoch Step: 1700 Loss: 25.568270 Tokens per Sec: 27000.718329\n",
      "Epoch Step: 1800 Loss: 30.169353 Tokens per Sec: 26886.974186\n",
      "Epoch Step: 1900 Loss: 29.092104 Tokens per Sec: 26743.169517\n",
      "Epoch Step: 2000 Loss: 50.285957 Tokens per Sec: 26745.829351\n",
      "Epoch Step: 2100 Loss: 28.049152 Tokens per Sec: 26899.431799\n",
      "Epoch Step: 2200 Loss: 15.806659 Tokens per Sec: 26484.283139\n",
      "\n",
      "Example #1\n",
      "Src :  als ich 11 jahre alt war , wurde ich eines morgens von den <unk> heller freude geweckt .\n",
      "Trg :  when i was 11 , i remember waking up one morning to the sound of joy in my house .\n",
      "Pred:  when i was 11 years old , i was a forger of the <unk> <unk> delight .\n",
      "\n",
      "Example #2\n",
      "Src :  mein vater hörte sich auf seinem kleinen , grauen radio die <unk> der bbc an .\n",
      "Trg :  my father was listening to bbc news on his small , gray radio .\n",
      "Pred:  my dad listened to his little , <unk> radio radio <unk> of the bbc .\n",
      "\n",
      "Example #3\n",
      "Src :  er sah sehr glücklich aus , was damals ziemlich ungewöhnlich war , da ihn die nachrichten meistens <unk> .\n",
      "Trg :  there was a big smile on his face which was unusual then , because the news mostly depressed him .\n",
      "Pred:  he looked very happy to see what was unusual at the time , because it was the most innovative most <unk> .\n",
      "\n",
      "Validation perplexity: 12.061666\n"
     ]
    }
   ],
   "source": [
    "mpath = Path('.')/'models'\n",
    "\n",
    "for epoch in range(10):\n",
    "    print(\"Epoch %d\" % epoch)\n",
    "    model.train()\n",
    "    run_epoch((rebatch(PAD_INDEX, b) for b in train_iter), \n",
    "              model, SimpleLossCompute(model.generator, criterion, optim), print_every=print_every)\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad(): \n",
    "        print_examples((rebatch(PAD_INDEX, x) for x in valid_iter), \n",
    "                           model, n=3, src_vocab=SRC.vocab, trg_vocab=TRG.vocab)        \n",
    "\n",
    "        dev_perplexity = run_epoch((rebatch(PAD_INDEX, b) for b in valid_iter), \n",
    "                                       model, \n",
    "                                       SimpleLossCompute(model.generator, criterion, None))\n",
    "        print(\"Validation perplexity: %f\" % dev_perplexity)\n",
    "        dev_perplexities.append(dev_perplexity)\n",
    "        \n",
    "        #mfile = mpath/f'de-en-trans-task.pth-{epoch}'\n",
    "        #torch.save(model.state_dict(), mfile)\n",
    "\n",
    "mfile = mpath/'de-en-trans-task.pth'\n",
    "torch.save(model.state_dict(), mfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_perplexity(dev_perplexities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example #1\n",
      "Src :  als ich 11 jahre alt war , wurde ich eines morgens von den <unk> heller freude geweckt .\n",
      "Trg :  when i was 11 , i remember waking up one morning to the sound of joy in my house .\n",
      "Pred:  when i was 11 years old , i was a forger of the <unk> <unk> delight .\n",
      "\n",
      "Example #2\n",
      "Src :  mein vater hörte sich auf seinem kleinen , grauen radio die <unk> der bbc an .\n",
      "Trg :  my father was listening to bbc news on his small , gray radio .\n",
      "Pred:  my dad listened to his little , <unk> radio radio <unk> of the bbc .\n",
      "\n",
      "Example #3\n",
      "Src :  er sah sehr glücklich aus , was damals ziemlich ungewöhnlich war , da ihn die nachrichten meistens <unk> .\n",
      "Trg :  there was a big smile on his face which was unusual then , because the news mostly depressed him .\n",
      "Pred:  he looked very happy to see what was unusual at the time , because it was the most innovative most <unk> .\n",
      "\n",
      "Example #4\n",
      "Src :  er rief : \" die taliban sind weg ! \"\n",
      "Trg :  \" the taliban are gone ! \" my father <unk> .\n",
      "Pred:  he called \" the taliban are gone . \"\n",
      "\n",
      "Example #5\n",
      "Src :  ich wusste nicht , was das bedeutete , aber es machte meinen vater offensichtlich sehr , sehr glücklich .\n",
      "Trg :  i did n't know what it meant , but i could see that my father was very , very happy .\n",
      "Pred:  i did n't know what that meant , but it was my father , very happy , very happy .\n",
      "\n",
      "Example #6\n",
      "Src :  \" jetzt kannst du auf eine richtige schule gehen , \" sagte er .\n",
      "Trg :  \" you can go to a real school now , \" he said .\n",
      "Pred:  \" now you can go on a right school , \" he said .\n",
      "\n",
      "Example #7\n",
      "Src :  diesen morgen werde ich niemals vergessen .\n",
      "Trg :  a morning that i will never forget .\n",
      "Pred:  i 'm going to get stuck that tomorrow .\n",
      "\n",
      "Example #8\n",
      "Src :  eine richtige schule .\n",
      "Trg :  a real school .\n",
      "Pred:  one right school .\n",
      "\n",
      "Example #9\n",
      "Src :  die taliban ergriffen die macht in afghanistan , als ich sechs war , und verboten es mädchen , zur schule zu gehen .\n",
      "Trg :  you see , i was six when the taliban took over afghanistan and made it illegal for girls to go to school .\n",
      "Pred:  the taliban is the power of afghanistan in afghanistan when i was six , and i banned it to go to school .\n",
      "\n",
      "Example #10\n",
      "Src :  nur so konnten wir beide zur schule gehen .\n",
      "Trg :  it was the only way we both could be educated .\n",
      "Pred:  so we could go both at school .\n",
      "\n",
      "Example #11\n",
      "Src :  jeden tag nahmen wir einen anderen weg , sodass niemand erraten konnte , wohin wir gingen .\n",
      "Trg :  each day , we took a different route so that no one would suspect where we were going .\n",
      "Pred:  every day we took a different way , so nobody could be able to go where we went .\n",
      "\n",
      "Example #12\n",
      "Src :  wir versteckten unsere bücher in <unk> , damit es so aussah , als würden wir nur einkaufen gehen .\n",
      "Trg :  we would cover our books in grocery bags so it would seem we were just out shopping .\n",
      "Pred:  we hid our books in <unk> so that it looked like we only only go to go .\n",
      "\n",
      "Example #13\n",
      "Src :  unterrichtet wurden wir in einem haus , über 100 mädchen in einem kleinen wohnzimmer .\n",
      "Trg :  the school was in a house , more than 100 of us packed in one small living room .\n",
      "Pred:  we were in a house , in 100 , in 100 inches in a little living room .\n",
      "\n",
      "Example #14\n",
      "Src :  im winter war es gemütlich , aber im sommer war es unglaublich heiß .\n",
      "Trg :  it was <unk> in winter but extremely hot in summer .\n",
      "Pred:  in winter , it was <unk> , but in summer , it was incredibly hot .\n",
      "\n",
      "Example #15\n",
      "Src :  wir alle wussten , dass wir unser leben <unk> : lehrer , schüler und unsere eltern .\n",
      "Trg :  we all knew we were <unk> our lives -- the teacher , the students and our parents .\n",
      "Pred:  we all knew that we live our lives , and teachers , students and our parents .\n",
      "\n",
      "Example #16\n",
      "Src :  immer wieder musste der unterricht plötzlich für eine woche ausfallen , weil die taliban verdacht <unk> hatten .\n",
      "Trg :  from time to time , the school would suddenly be <unk> for a week because taliban were suspicious .\n",
      "Pred:  and again , they had to have the <unk> of a week for a week because the taliban 's <unk> <unk> .\n",
      "\n",
      "Example #17\n",
      "Src :  wir waren uns nie sicher , wie viel sie über uns wussten .\n",
      "Trg :  we always wondered what they knew about us .\n",
      "Pred:  we never been sure how much they knew about us .\n",
      "\n",
      "Example #18\n",
      "Src :  verfolgten sie uns ?\n",
      "Trg :  were we being followed ?\n",
      "Pred:  do you get us ?\n",
      "\n",
      "Example #19\n",
      "Src :  wussten sie , wo wir wohnen ?\n",
      "Trg :  do they know where we live ?\n",
      "Pred:  did you know where we live ?\n",
      "\n",
      "Example #20\n",
      "Src :  wir hatten angst , aber wir wollten trotzdem zur schule gehen .\n",
      "Trg :  we were scared , but still , school was where we wanted to be .\n",
      "Pred:  we had scared , but we wanted to go to school .\n",
      "\n",
      "Example #21\n",
      "Src :  ich hatte großes glück in einer familie <unk> , in der bildung als wichtig galt und töchter geschätzt wurden .\n",
      "Trg :  i was very lucky to grow up in a family where education was <unk> and daughters were <unk> .\n",
      "Pred:  i was lucky in a family of a family in education , and it was <unk> to <unk> and daughters .\n",
      "\n",
      "Example #22\n",
      "Src :  mein großvater war seiner zeit weit voraus .\n",
      "Trg :  my grandfather was an extraordinary man for his time .\n",
      "Pred:  my grandfather was far beyond my mind .\n",
      "\n",
      "Example #23\n",
      "Src :  meine gebildete mutter aber wurde lehrerin .\n",
      "Trg :  but my educated mother became a teacher .\n",
      "Pred:  my mother was a teacher .\n",
      "\n",
      "Example #24\n",
      "Src :  das ist sie .\n",
      "Trg :  there she is .\n",
      "Pred:  that 's her .\n",
      "\n",
      "Example #25\n",
      "Src :  und mein vater – hier zu sehen – war der erste in seiner familie , der jemals eine schulbildung erhielt .\n",
      "Trg :  and my father -- that 's him -- he was the first ever in his family to receive an education .\n",
      "Pred:  and my father -- to see here -- was the first in his family , who had a clue .\n",
      "\n",
      "Example #26\n",
      "Src :  er sah es als ein viel größeres risiko an , seine kinder nicht zur schule zu schicken .\n",
      "Trg :  to him , there was greater risk in not educating his children .\n",
      "Pred:  he saw it as a much risk of the risk of his children , his children not to send them to school .\n",
      "\n",
      "Example #27\n",
      "Src :  dein geld kann gestohlen werden . du kannst im krieg aus deinem haus vertrieben werden .\n",
      "Trg :  your money can be stolen . you can be forced to leave your home during a war .\n",
      "Pred:  your hand can be stolen . you can be in the war of your house .\n",
      "\n",
      "Example #28\n",
      "Src :  also – willst du immer noch aufgeben ? \"\n",
      "Trg :  so do you still not want to continue ? \"\n",
      "Pred:  so , you want to get more and more ? \"\n",
      "\n",
      "Example #29\n",
      "Src :  heute bin ich 22 jahre alt .\n",
      "Trg :  today i am 22 .\n",
      "Pred:  i 'm 22 years old .\n",
      "\n",
      "Example #30\n",
      "Src :  ich bin in einem land aufgewachsen , das durch jahrzehnte des krieges zerstört worden ist .\n",
      "Trg :  i was raised in a country that has been destroyed by decades of war .\n",
      "Pred:  i grew up in a country that was destroyed by decades of war .\n",
      "\n",
      "Example #31\n",
      "Src :  stattdessen stehe ich heute hier , als <unk> <unk> des <unk> college .\n",
      "Trg :  instead , i stand here a proud graduate of <unk> college .\n",
      "Pred:  instead , i 'm here today to be here as a <unk> <unk> of the college college .\n",
      "\n",
      "Example #32\n",
      "Src :  meine familie glaubt an mich .\n",
      "Trg :  my family believes in me .\n",
      "Pred:  my family believes me .\n",
      "\n",
      "Example #33\n",
      "Src :  ich habe große träume , aber meine familie hat noch größere träume für mich .\n",
      "Trg :  i dream big , but my family dreams even bigger for me .\n",
      "Pred:  i have a big dreams , but my family has bigger bigger dreams for me .\n",
      "\n",
      "Example #34\n",
      "Src :  deshalb bin ich globale <unk> für <unk> , einer weltweiten kampagne für die bildung von frauen .\n",
      "Trg :  that 's why i am a global ambassador for <unk> , a global campaign to educate women .\n",
      "Pred:  so i 'm global health care for <unk> , a global campaign for women 's education .\n",
      "\n",
      "Example #35\n",
      "Src :  es ist wundervoll zu sehen , wie die <unk> an meiner schule mit großem ehrgeiz alle ihnen <unk> chancen wahrnehmen möchten .\n",
      "Trg :  the exciting thing is that i see students at my school with ambition <unk> at opportunity .\n",
      "Pred:  it 's wonderful to see how the <unk> <unk> of my school with my friends are <unk> to <unk> more opportunities for them .\n",
      "\n",
      "Example #36\n",
      "Src :  das soll nicht heißen , dass unsere mütter keine wichtige rolle in unserem erfolg spielen .\n",
      "Trg :  it 's not to say that our mothers are n't key in our success .\n",
      "Pred:  this is not that our mothers do n't want to play in our success .\n",
      "\n",
      "Example #37\n",
      "Src :  unter den taliban gingen nur wenige hundert mädchen zur schule – denn es war ja illegal .\n",
      "Trg :  under the taliban , girls who went to school <unk> in the hundreds -- remember , it was illegal .\n",
      "Pred:  there 's only a few hundred girls for school , because it was illegal .\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example #38\n",
      "Src :  heute jedoch drücken in afghanistan über drei millionen mädchen die <unk> .\n",
      "Trg :  but today , more than three million girls are in school in afghanistan .\n",
      "Pred:  but in the three days , we have three million girls who are <unk> .\n",
      "\n",
      "Example #39\n",
      "Src :  afghanistan erscheint , von amerika aus betrachtet , so anders .\n",
      "Trg :  afghanistan looks so different from here in america .\n",
      "Pred:  afghanistan is from the <unk> , from this .\n",
      "\n",
      "Example #40\n",
      "Src :  die amerikaner erkennen , wie unsicher solche veränderungen sind .\n",
      "Trg :  i find that americans see the <unk> in changes .\n",
      "Pred:  the americans who are the fundamental technologies such changes .\n",
      "\n",
      "Example #41\n",
      "Src :  ich befürchte , dass die veränderungen nicht von dauer sind und sich mit dem abzug der <unk> wieder alles ändert .\n",
      "Trg :  i fear that these changes will not last much beyond the u.s. troops ' <unk> .\n",
      "Pred:  i 'm saying that changes the change are not working from the <unk> and over the trigger of the <unk> back .\n",
      "\n",
      "Example #42\n",
      "Src :  genau wie ich haben sie große träume .\n",
      "Trg :  like me , they are dreaming big .\n",
      "Pred:  just like me , you have a great job .\n",
      "\n",
      "Example #43\n",
      "Src :  vielen dank .\n",
      "Trg :  thank you .\n",
      "Pred:  thank you .\n",
      "\n",
      "Example #44\n",
      "Src :  ich habe für eine italienische <unk> gearbeitet und jedes einzelne projekt , das wir auf die beine stellten , scheiterte .\n",
      "Trg :  i worked for an italian ngo , and every single project that we set up in africa failed .\n",
      "Pred:  i worked on a italian <unk> and i just joined every single project we 're <unk> on the legs .\n",
      "\n",
      "Example #45\n",
      "Src :  ich war verzweifelt .\n",
      "Trg :  and i was <unk> .\n",
      "Pred:  i was desperate .\n",
      "\n",
      "Example #46\n",
      "Src :  ich , 21 jahre , dachte , dass wir italiener gute menschen sind und gute arbeit in afrika <unk> .\n",
      "Trg :  i thought , age 21 , that we italians were good people and we were doing good work in africa .\n",
      "Pred:  i 'm 21 years , thinking that we 're good people and good jobs in africa .\n",
      "\n",
      "Example #47\n",
      "Src :  stattdessen töteten wir alles , was wir <unk> .\n",
      "Trg :  instead , everything we touched we killed .\n",
      "Pred:  instead , we 've all done what we 're doing .\n",
      "\n",
      "Example #48\n",
      "Src :  wir waren erstaunt , dass es in einem so <unk> tal keine landwirtschaft gab .\n",
      "Trg :  and we were amazed that the local people , in such a fertile valley , would not have any agriculture .\n",
      "Pred:  we were amazed that there was no <unk> valley in the same valley .\n",
      "\n",
      "Example #49\n",
      "Src :  aber anstatt zu fragen , warum sie nichts <unk> , sagten wir einfach \" gott sei dank , dass wir hier sind ! \"\n",
      "Trg :  but instead of asking them how come they were not growing anything , we simply said , \" thank god we 're here . \"\n",
      "Pred:  but instead of asking you , nothing , we said , \" thank god , we 're here ! \"\n",
      "\n",
      "Example #50\n",
      "Src :  \" gerade noch rechtzeitig , um die menschen <unk> vor dem verhungern zu retten . \"\n",
      "Trg :  \" just in the nick of time to save the <unk> people from starvation . \"\n",
      "Pred:  \" just in the world to save people 's <unk> <unk> .\n",
      "\n",
      "Example #51\n",
      "Src :  natürlich <unk> alles wunderbar in afrika .\n",
      "Trg :  and of course , everything in africa grew beautifully .\n",
      "Pred:  of course , all of the things in africa is all in africa .\n",
      "\n",
      "Example #52\n",
      "Src :  wir hatten diese <unk> tomaten . in italien wurden sie so groß , in sambia so groß .\n",
      "Trg :  we had these magnificent tomatoes . in italy , a tomato would grow to this size . in zambia , to this size .\n",
      "Pred:  we had this <unk> tomatoes . in italy , they were so big , in the way .\n",
      "\n",
      "Example #53\n",
      "Src :  wir konnten es nicht glauben und sagten den <unk> : \" schaut , wie einfach landwirtschaft ist . \"\n",
      "Trg :  and we could not believe , and we were telling the <unk> , \" look how easy agriculture is . \"\n",
      "Pred:  we could n't believe it and said , \" the <unk> : \" look , just like \" agriculture . \"\n",
      "\n",
      "Example #54\n",
      "Src :  als die tomaten reif und rot waren , kamen über nacht etwa 200 <unk> aus dem fluss und <unk> alles .\n",
      "Trg :  when the tomatoes were nice and ripe and red , overnight , some 200 <unk> came out from the river and they ate everything .\n",
      "Pred:  when the tomatoes of the <unk> and <unk> were , came about the <unk> of the <unk> of the river and <unk> everything .\n",
      "\n",
      "Example #55\n",
      "Src :  wir sagten zu den <unk> : \" oh gott , die <unk> ! \"\n",
      "Trg :  and we said to the <unk> , \" my god , the <unk> ! \"\n",
      "Pred:  we said to the <unk> , \" oh god , the <unk> ! \"\n",
      "\n",
      "Example #56\n",
      "Src :  und sie sagten : \" ja , deswegen haben wir keine landwirtschaft hier . \"\n",
      "Trg :  and the <unk> said , \" yes , that 's why we have no agriculture here . \"\n",
      "Pred:  and they said , \" yes , so we 're not a agriculture . \"\n",
      "\n",
      "Example #57\n",
      "Src :  \" warum habt ihr uns das nicht gesagt ? \" \" ihr habt uns niemals gefragt . \"\n",
      "Trg :  \" why did n't you tell us ? \" \" you never asked . \"\n",
      "Pred:  \" why did n't you tell us ? \" \" you 've never asked us . \"\n",
      "\n",
      "Example #58\n",
      "Src :  wir <unk> zumindest die <unk> .\n",
      "Trg :  because , you see , at least we fed the <unk> .\n",
      "Pred:  we <unk> at the <unk> .\n",
      "\n",
      "Example #59\n",
      "Src :  sie sollten den unsinn sehen – – sie sollten den unsinn sehen , den wir den <unk> afrikanischen menschen beschert haben .\n",
      "Trg :  you should see the rubbish — -- you should see the rubbish that we have <unk> on <unk> african people .\n",
      "Pred:  you should see the nonsense -- -- you should see the nonsense of us that we see the african african african people .\n",
      "\n",
      "Example #60\n",
      "Src :  sie sollten das buch \" dead <unk> \" von <unk> <unk> lesen , sie ist eine <unk> <unk> .\n",
      "Trg :  you want to read the book , read \" dead aid , \" by <unk> <unk> , <unk> woman economist .\n",
      "Pred:  you should be the book of \" <unk> \" <unk> by <unk> <unk> , she 's a <unk> <unk> .\n",
      "\n",
      "Example #61\n",
      "Src :  das buch wurde 2009 veröffentlicht .\n",
      "Trg :  the book was published in 2009 .\n",
      "Pred:  the book was published in 2009 .\n",
      "\n",
      "Example #62\n",
      "Src :  wir <unk> haben dem afrikanischen kontinent 1,5 billionen euro in den letzten 50 jahren gegeben .\n",
      "Trg :  we western donor countries have given the african continent two trillion american dollars in the last 50 years .\n",
      "Pred:  we 've been able to 1.5 billion african billion dollars in the last 50 years .\n",
      "\n",
      "Example #63\n",
      "Src :  ich werde ihnen nicht erzählen , was dieses geld <unk> hat .\n",
      "Trg :  i 'm not going to tell you the damage that that money has done .\n",
      "Pred:  i 'm not going to tell you what this money has done .\n",
      "\n",
      "Example #64\n",
      "Src :  lesen sie einfach ihr buch .\n",
      "Trg :  just go and read her book .\n",
      "Pred:  just read your book .\n",
      "\n",
      "Example #65\n",
      "Src :  lesen sie von einer <unk> , was wir <unk> haben .\n",
      "Trg :  read it from an african woman , the damage that we have done .\n",
      "Pred:  read them , from a <unk> , what we have .\n",
      "\n",
      "Example #66\n",
      "Src :  beide wörter stammen von der <unk> wurzel \" pater \" , was \" vater \" bedeutet .\n",
      "Trg :  the two words come from the latin root \" <unk> , \" which means \" father . \"\n",
      "Pred:  both words were borrowed by the most beautifully root \" <unk> , \" what 's the \" <unk> . \"\n",
      "\n",
      "Example #67\n",
      "Src :  aber sie haben zwei verschiedene bedeutungen .\n",
      "Trg :  but they mean two different things .\n",
      "Pred:  but you have two different meanings .\n",
      "\n",
      "Example #68\n",
      "Src :  <unk> : ich <unk> jeden einer anderen kultur , als wären sie meine kinder . \" ich liebe euch so sehr . \"\n",
      "Trg :  <unk> , i treat anybody from a different culture as if they were my children . \" i love you so much . \"\n",
      "Pred:  <unk> : i 'm going to culture every other culture when they were my children . \" i love you so much . \"\n",
      "\n",
      "Example #69\n",
      "Src :  <unk> : ich <unk> jeden einer anderen kultur , als wären sie meine <unk> .\n",
      "Trg :  <unk> , i treat everybody from another culture as if they were my servants .\n",
      "Pred:  <unk> : i 'm going to find every other culture , when they were my <unk> .\n",
      "\n",
      "Example #70\n",
      "Src :  deshalb werden weiße menschen in afrika \" <unk> \" , chef , genannt .\n",
      "Trg :  that 's why the white people in africa are called \" <unk> , \" boss .\n",
      "Pred:  so that 's why white people in africa is called \" <unk> , \" . \"\n",
      "\n",
      "Example #71\n",
      "Src :  dies sollte das erste prinzip der hilfe sein .\n",
      "Trg :  this should be the first principle of aid .\n",
      "Pred:  this should be the first principle of aid .\n",
      "\n",
      "Example #72\n",
      "Src :  das erste prinzip der hilfe ist respekt .\n",
      "Trg :  the first principle of aid is respect .\n",
      "Pred:  the first principle of the aid is respect .\n",
      "\n",
      "Example #73\n",
      "Src :  was man macht – man hält den mund .\n",
      "Trg :  so what you do -- you shut up .\n",
      "Pred:  what you do -- you 're holding the mouths .\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example #74\n",
      "Src :  man erreicht niemals eine gemeinde mit ideen , man setzt sich mit den <unk> zusammen .\n",
      "Trg :  you never arrive in a community with any ideas , and you sit with the local people .\n",
      "Pred:  you never have a community with ideas , you 're going to <unk> together .\n",
      "\n",
      "Example #75\n",
      "Src :  wir arbeiten nicht von büros aus .\n",
      "Trg :  we do n't work from offices .\n",
      "Pred:  we do n't work from offices .\n",
      "\n",
      "Example #76\n",
      "Src :  wir treffen uns in <unk> . wir treffen uns in <unk> .\n",
      "Trg :  we meet at the cafe . we meet at the pub .\n",
      "Pred:  we 're in <unk> . we meeting in <unk> .\n",
      "\n",
      "Example #77\n",
      "Src :  wir haben keine infrastruktur .\n",
      "Trg :  we have zero infrastructure .\n",
      "Pred:  we have no infrastructure .\n",
      "\n",
      "Example #78\n",
      "Src :  wir schließen freundschaften und finden heraus , was die person möchte .\n",
      "Trg :  and what we do , we become friends , and we find out what that person wants to do .\n",
      "Pred:  we close them and find out what the person would want .\n",
      "\n",
      "Example #79\n",
      "Src :  das wichtigste ist die leidenschaft .\n",
      "Trg :  the most important thing is passion .\n",
      "Pred:  the most important thing is the passion .\n",
      "\n",
      "Example #80\n",
      "Src :  man kann jemandem eine idee vermitteln .\n",
      "Trg :  you can give somebody an idea .\n",
      "Pred:  you can get an idea .\n",
      "\n",
      "Example #81\n",
      "Src :  wenn diese person diese nicht mag , was soll man tun ?\n",
      "Trg :  if that person does n't want to do it , what are you going to do ?\n",
      "Pred:  if these person do n't like this , what do you do ?\n",
      "\n",
      "Example #82\n",
      "Src :  die <unk> für das eigene wachstum der person ist das wichtigste .\n",
      "Trg :  the passion that the person has for her own growth is the most important thing .\n",
      "Pred:  the <unk> of the person is the most important thing .\n",
      "\n",
      "Example #83\n",
      "Src :  die <unk> für das eigene wachsen ist das wichtigste der menschheit .\n",
      "Trg :  the passion that that man has for his own personal growth is the most important thing .\n",
      "Pred:  the <unk> for the individual is the most important of the human .\n",
      "\n",
      "Example #84\n",
      "Src :  wir helfen ihnen , das wissen zu finden , denn niemand kann allein erfolgreich sein .\n",
      "Trg :  and then we help them to go and find the knowledge , because nobody in the world can succeed alone .\n",
      "Pred:  we 're going to help you know that , nobody can be successful alone .\n",
      "\n",
      "Example #85\n",
      "Src :  die person mit der idee hat vielleicht nicht das wissen , doch es ist verfügbar .\n",
      "Trg :  the person with the idea may not have the knowledge , but the knowledge is available .\n",
      "Pred:  the person with the idea may not know that , but it 's available .\n",
      "\n",
      "Example #86\n",
      "Src :  lassen sie mich ihnen ein geheimnis verraten .\n",
      "Trg :  let me tell you a secret .\n",
      "Pred:  let me tell you a secret .\n",
      "\n",
      "Example #87\n",
      "Src :  es gibt ein problem mit <unk> .\n",
      "Trg :  there is a problem with community meetings .\n",
      "Pred:  there 's a problem with <unk> .\n",
      "\n",
      "Example #88\n",
      "Src :  planung hat diesen blinden fleck .\n",
      "Trg :  so planning has this blind spot .\n",
      "Pred:  planning has this blind spot .\n",
      "\n",
      "Example #89\n",
      "Src :  die <unk> menschen der gemeinde kennt man nicht , weil sie nie zu öffentlichen treffen erscheinen .\n",
      "Trg :  the smartest people in your community you do n't even know , because they do n't come to your public meetings .\n",
      "Pred:  the <unk> of the people you do n't know the community because they never <unk> to the public meeting .\n",
      "\n",
      "Example #90\n",
      "Src :  ein neuer beruf muss geschaffen werden .\n",
      "Trg :  you have to create a new profession .\n",
      "Pred:  a new particle needs to be created .\n",
      "\n",
      "Example #91\n",
      "Src :  ich habe dies in <unk> , <unk> , versucht .\n",
      "Trg :  i started this as a <unk> in <unk> , in western australia .\n",
      "Pred:  i 've done this in <unk> , <unk> , tried .\n",
      "\n",
      "Example #92\n",
      "Src :  wie machen sie ... ? \" ich sagte : \" ich mache etwas sehr , sehr schwieriges .\n",
      "Trg :  how can you do — ? \" and i said , \" i do something very , very , very difficult .\n",
      "Pred:  how do you do ? \" i said , \" i do something rather , very hard .\n",
      "\n",
      "Example #93\n",
      "Src :  ich halte den mund und höre ihnen zu . \"\n",
      "Trg :  i shut up , and listen to them . \"\n",
      "Pred:  i 'm going to go down and make you . \"\n",
      "\n",
      "Example #94\n",
      "Src :  also – – also sagt die regierung : \" machen sie es nochmal . \"\n",
      "Trg :  so — — so the government says , \" do it again . \"\n",
      "Pred:  so -- -- so what 's the government : \" do you make it again . \"\n",
      "\n",
      "Example #95\n",
      "Src :  wir haben es weltweit in 300 gemeinden gemacht .\n",
      "Trg :  we 've done it in 300 communities around the world .\n",
      "Pred:  we 've done it in 300 communities .\n",
      "\n",
      "Example #96\n",
      "Src :  wir haben 40 000 unternehmen bei der gründung geholfen .\n",
      "Trg :  we have helped to start 40,000 businesses .\n",
      "Pred:  we 've helped 40 people in the gulf .\n",
      "\n",
      "Example #97\n",
      "Src :  es gibt eine neue generation von unternehmen , die an einsamkeit vergehen .\n",
      "Trg :  there is a new generation of entrepreneurs who are dying of <unk> .\n",
      "Pred:  there 's a new generation of companies that are <unk> .\n",
      "\n",
      "Example #98\n",
      "Src :  peter drucker , einer der besten <unk> der geschichte , starb mit 96 vor einigen jahren .\n",
      "Trg :  peter <unk> , one of the greatest management consultants in history , died age 96 , a few years ago .\n",
      "Pred:  peter ward , one of the best friend of the story , died with a couple of years ago .\n",
      "\n",
      "Example #99\n",
      "Src :  planung ist der <unk> des <unk> .\n",
      "Trg :  planning is the kiss of death of entrepreneurship .\n",
      "Pred:  planning is the <unk> of the <unk> .\n",
      "\n",
      "Example #100\n",
      "Src :  man muss lernen , wie man diese dazu bringt , auf einen <unk> .\n",
      "Trg :  you have to learn how to get these people to come and talk to you .\n",
      "Pred:  you need to learn how to make it on a <unk> .\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_examples((rebatch(PAD_INDEX, x) for x in valid_iter), \n",
    "                           model, n=100, src_vocab=SRC.vocab, trg_vocab=TRG.vocab) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
