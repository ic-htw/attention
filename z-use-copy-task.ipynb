{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from data_gen import data_gen\n",
    "from model_seq2seq import Seq2SeqModel\n",
    "from model_generator import Generator\n",
    "from simple_loss_compute import SimpleLossCompute\n",
    "from run_epoch import run_epoch\n",
    "from print_examples import print_examples\n",
    "from iclib.core import init_seed, pt_init, pt_use_cuda, pt_device\n",
    "from iclib.print_info import set_print, toggle_print, p_hb, p_he, p_ti"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA: True\n",
      "DEVICE: cuda:0\n"
     ]
    }
   ],
   "source": [
    "pt_init()\n",
    "#pt_init(use_gpu=False)\n",
    "print(\"CUDA:\", pt_use_cuda())\n",
    "print(\"DEVICE:\", pt_device())\n",
    "\n",
    "init_seed(42)\n",
    "set_print(False)\n",
    "\n",
    "V = 11\n",
    "E = 32\n",
    "H = 64\n",
    "L = 1\n",
    "dropout = 0.1\n",
    "\n",
    "S = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Seq2SeqModel(V, V, E, E, H, L, dropout, Generator(H, V))\n",
    "\n",
    "mpath = Path('.')/'models'\n",
    "mfile = mpath/'copy-task.pth'\n",
    "model.load_state_dict(torch.load(mfile))\n",
    "\n",
    "if pt_use_cuda():\n",
    "    model.cuda(pt_device())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example #1\n",
      "Src :  1 7 7 8 5 3 8 6 3\n",
      "Trg :  1 7 7 8 5 3 8 6 3\n",
      "Pred:  1 7 7 8 5 3 8 6 3\n",
      "\n",
      "Example #2\n",
      "Src :  3 5 3 1 5 10 7 7 9\n",
      "Trg :  3 5 3 1 5 10 7 7 9\n",
      "Pred:  3 5 3 1 5 10 7 7 9\n",
      "\n",
      "Example #3\n",
      "Src :  10 3 7 1 4 4 5 7 7\n",
      "Trg :  10 3 7 1 4 4 5 7 7\n",
      "Pred:  10 3 7 1 4 4 5 7 7\n",
      "\n",
      "Example #4\n",
      "Src :  7 3 6 2 10 9 5 6 4\n",
      "Trg :  7 3 6 2 10 9 5 6 4\n",
      "Pred:  7 3 6 2 10 9 5 6 4\n",
      "\n",
      "Example #5\n",
      "Src :  7 9 7 1 1 9 9 4 9\n",
      "Trg :  7 9 7 1 1 9 9 4 9\n",
      "Pred:  7 9 7 1 9 1 9 4 9\n",
      "\n",
      "Example #6\n",
      "Src :  7 6 8 9 5 1 3 10 8\n",
      "Trg :  7 6 8 9 5 1 3 10 8\n",
      "Pred:  7 6 8 9 5 1 3 10 8\n",
      "\n",
      "Example #7\n",
      "Src :  8 9 4 1 1 10 4 7 2\n",
      "Trg :  8 9 4 1 1 10 4 7 2\n",
      "Pred:  8 9 4 1 1 10 4 7 2\n",
      "\n",
      "Example #8\n",
      "Src :  1 5 1 8 1 1 2 2 6\n",
      "Trg :  1 5 1 8 1 1 2 2 6\n",
      "Pred:  1 5 1 8 1 1 2 2 6\n",
      "\n",
      "Example #9\n",
      "Src :  5 1 1 3 2 5 10 6 7\n",
      "Trg :  5 1 1 3 2 5 10 6 7\n",
      "Pred:  5 1 1 3 2 5 10 6 7\n",
      "\n",
      "Example #10\n",
      "Src :  7 8 1 6 8 5 4 2 6\n",
      "Trg :  7 8 1 6 8 5 4 2 6\n",
      "Pred:  7 8 1 6 8 5 4 2 6\n",
      "\n"
     ]
    }
   ],
   "source": [
    "eval_data = list(data_gen(num_words=V, batch_size=1, num_batches=10))\n",
    "print_examples(eval_data, model, n=100, max_len=9)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
