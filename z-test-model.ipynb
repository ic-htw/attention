{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "from iclib.core import init_seed, pt_init, pt_use_cuda, pt_device\n",
    "from iclib.print_info import set_print, toggle_print, p_hb, p_he, p_ti\n",
    "from data_gen import data_gen\n",
    "from model_seq2seq import Seq2SeqModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA: False\n",
      "DEVICE: cpu:0\n"
     ]
    }
   ],
   "source": [
    "#pt_init()\n",
    "pt_init(use_gpu=False)\n",
    "print(\"CUDA:\", pt_use_cuda())\n",
    "print(\"DEVICE:\", pt_device())\n",
    "\n",
    "init_seed(42)\n",
    "set_print(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/iclassen/app/anaconda3/envs/fastai/lib/python3.6/site-packages/torch/nn/modules/rnn.py:46: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "  \"num_layers={}\".format(dropout, num_layers))\n"
     ]
    }
   ],
   "source": [
    "V = 11\n",
    "E = 32\n",
    "B = 2\n",
    "S = 10\n",
    "H = 64\n",
    "L = 1\n",
    "dropout = 0.1\n",
    "init_seed(42)\n",
    "data = list(data_gen(num_words=V, batch_size=B, num_batches=3, length=S))\n",
    "model = Seq2SeqModel(V, V, E, E, H, L, dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BEGIN forward ----------------------------------------\n",
      "s - dim: 2, size: [2, 9] - [B,Ss]\n",
      "BEGIN forward_encode ----------------------------------------\n",
      "embs_s - dim: 3, size: [2, 9, 32] - [B,Ss,Es]\n",
      "END forward_encode ----------------------------------------\n",
      "enc_hidden - dim: 3, size: [2, 9, 128] - [B,Ss,2H]\n",
      "enc_final - dim: 3, size: [1, 2, 128] - [L,B,2H]\n",
      "BEGIN forward_decode ----------------------------------------\n",
      "mask_s - dim: 3, size: [2, 1, 9]\n",
      "mask_t - dim: 2, size: [2, 9]\n",
      "dec_hidden - dim: 3, size: [1, 2, 64] - [L,B,H]\n",
      "proj_key - dim: 3, size: [2, 9, 64] - [B,Ss,H]\n",
      "embs_t - dim: 3, size: [2, 9, 32] - [B,St,Et]\n",
      "BEGIN forward_step ----------------------------------------\n",
      "query - dim: 3, size: [2, 1, 64] - [B,1,H]\n",
      "BEGIN forward_attention ----------------------------------------\n",
      "query - dim: 3, size: [2, 1, 64] - [B,1,H]\n",
      "scores - dim: 3, size: [2, 9, 1] - [B,Ss,1]\n",
      "scores - dim: 3, size: [2, 1, 9] - [B,1,Ss]\n",
      "alphas - dim: 3, size: [2, 1, 9] - [B,1,Ss]\n",
      "context - dim: 3, size: [2, 1, 128] - [B,1,2H]\n",
      "END forward_attention ----------------------------------------\n",
      "decoder_input - dim: 3, size: [2, 1, 160] - [B,1,Et+2H]\n",
      "output - dim: 3, size: [2, 1, 64] - [B,1,H]\n",
      "dec_hidden - dim: 3, size: [1, 2, 64] - [L*directions,B,H]\n",
      "pre_output - dim: 3, size: [2, 1, 224] - [B,1,Et+H+2H]\n",
      "pre_output - dim: 3, size: [2, 1, 64] - [B,1,H]\n",
      "END forward_step ----------------------------------------\n",
      "BEGIN forward_step ----------------------------------------\n",
      "query - dim: 3, size: [2, 1, 64] - [B,1,H]\n",
      "BEGIN forward_attention ----------------------------------------\n",
      "query - dim: 3, size: [2, 1, 64] - [B,1,H]\n",
      "scores - dim: 3, size: [2, 9, 1] - [B,Ss,1]\n",
      "scores - dim: 3, size: [2, 1, 9] - [B,1,Ss]\n",
      "alphas - dim: 3, size: [2, 1, 9] - [B,1,Ss]\n",
      "context - dim: 3, size: [2, 1, 128] - [B,1,2H]\n",
      "END forward_attention ----------------------------------------\n",
      "decoder_input - dim: 3, size: [2, 1, 160] - [B,1,Et+2H]\n",
      "output - dim: 3, size: [2, 1, 64] - [B,1,H]\n",
      "dec_hidden - dim: 3, size: [1, 2, 64] - [L*directions,B,H]\n",
      "pre_output - dim: 3, size: [2, 1, 224] - [B,1,Et+H+2H]\n",
      "pre_output - dim: 3, size: [2, 1, 64] - [B,1,H]\n",
      "END forward_step ----------------------------------------\n",
      "BEGIN forward_step ----------------------------------------\n",
      "query - dim: 3, size: [2, 1, 64] - [B,1,H]\n",
      "BEGIN forward_attention ----------------------------------------\n",
      "query - dim: 3, size: [2, 1, 64] - [B,1,H]\n",
      "scores - dim: 3, size: [2, 9, 1] - [B,Ss,1]\n",
      "scores - dim: 3, size: [2, 1, 9] - [B,1,Ss]\n",
      "alphas - dim: 3, size: [2, 1, 9] - [B,1,Ss]\n",
      "context - dim: 3, size: [2, 1, 128] - [B,1,2H]\n",
      "END forward_attention ----------------------------------------\n",
      "decoder_input - dim: 3, size: [2, 1, 160] - [B,1,Et+2H]\n",
      "output - dim: 3, size: [2, 1, 64] - [B,1,H]\n",
      "dec_hidden - dim: 3, size: [1, 2, 64] - [L*directions,B,H]\n",
      "pre_output - dim: 3, size: [2, 1, 224] - [B,1,Et+H+2H]\n",
      "pre_output - dim: 3, size: [2, 1, 64] - [B,1,H]\n",
      "END forward_step ----------------------------------------\n",
      "BEGIN forward_step ----------------------------------------\n",
      "query - dim: 3, size: [2, 1, 64] - [B,1,H]\n",
      "BEGIN forward_attention ----------------------------------------\n",
      "query - dim: 3, size: [2, 1, 64] - [B,1,H]\n",
      "scores - dim: 3, size: [2, 9, 1] - [B,Ss,1]\n",
      "scores - dim: 3, size: [2, 1, 9] - [B,1,Ss]\n",
      "alphas - dim: 3, size: [2, 1, 9] - [B,1,Ss]\n",
      "context - dim: 3, size: [2, 1, 128] - [B,1,2H]\n",
      "END forward_attention ----------------------------------------\n",
      "decoder_input - dim: 3, size: [2, 1, 160] - [B,1,Et+2H]\n",
      "output - dim: 3, size: [2, 1, 64] - [B,1,H]\n",
      "dec_hidden - dim: 3, size: [1, 2, 64] - [L*directions,B,H]\n",
      "pre_output - dim: 3, size: [2, 1, 224] - [B,1,Et+H+2H]\n",
      "pre_output - dim: 3, size: [2, 1, 64] - [B,1,H]\n",
      "END forward_step ----------------------------------------\n",
      "BEGIN forward_step ----------------------------------------\n",
      "query - dim: 3, size: [2, 1, 64] - [B,1,H]\n",
      "BEGIN forward_attention ----------------------------------------\n",
      "query - dim: 3, size: [2, 1, 64] - [B,1,H]\n",
      "scores - dim: 3, size: [2, 9, 1] - [B,Ss,1]\n",
      "scores - dim: 3, size: [2, 1, 9] - [B,1,Ss]\n",
      "alphas - dim: 3, size: [2, 1, 9] - [B,1,Ss]\n",
      "context - dim: 3, size: [2, 1, 128] - [B,1,2H]\n",
      "END forward_attention ----------------------------------------\n",
      "decoder_input - dim: 3, size: [2, 1, 160] - [B,1,Et+2H]\n",
      "output - dim: 3, size: [2, 1, 64] - [B,1,H]\n",
      "dec_hidden - dim: 3, size: [1, 2, 64] - [L*directions,B,H]\n",
      "pre_output - dim: 3, size: [2, 1, 224] - [B,1,Et+H+2H]\n",
      "pre_output - dim: 3, size: [2, 1, 64] - [B,1,H]\n",
      "END forward_step ----------------------------------------\n",
      "BEGIN forward_step ----------------------------------------\n",
      "query - dim: 3, size: [2, 1, 64] - [B,1,H]\n",
      "BEGIN forward_attention ----------------------------------------\n",
      "query - dim: 3, size: [2, 1, 64] - [B,1,H]\n",
      "scores - dim: 3, size: [2, 9, 1] - [B,Ss,1]\n",
      "scores - dim: 3, size: [2, 1, 9] - [B,1,Ss]\n",
      "alphas - dim: 3, size: [2, 1, 9] - [B,1,Ss]\n",
      "context - dim: 3, size: [2, 1, 128] - [B,1,2H]\n",
      "END forward_attention ----------------------------------------\n",
      "decoder_input - dim: 3, size: [2, 1, 160] - [B,1,Et+2H]\n",
      "output - dim: 3, size: [2, 1, 64] - [B,1,H]\n",
      "dec_hidden - dim: 3, size: [1, 2, 64] - [L*directions,B,H]\n",
      "pre_output - dim: 3, size: [2, 1, 224] - [B,1,Et+H+2H]\n",
      "pre_output - dim: 3, size: [2, 1, 64] - [B,1,H]\n",
      "END forward_step ----------------------------------------\n",
      "BEGIN forward_step ----------------------------------------\n",
      "query - dim: 3, size: [2, 1, 64] - [B,1,H]\n",
      "BEGIN forward_attention ----------------------------------------\n",
      "query - dim: 3, size: [2, 1, 64] - [B,1,H]\n",
      "scores - dim: 3, size: [2, 9, 1] - [B,Ss,1]\n",
      "scores - dim: 3, size: [2, 1, 9] - [B,1,Ss]\n",
      "alphas - dim: 3, size: [2, 1, 9] - [B,1,Ss]\n",
      "context - dim: 3, size: [2, 1, 128] - [B,1,2H]\n",
      "END forward_attention ----------------------------------------\n",
      "decoder_input - dim: 3, size: [2, 1, 160] - [B,1,Et+2H]\n",
      "output - dim: 3, size: [2, 1, 64] - [B,1,H]\n",
      "dec_hidden - dim: 3, size: [1, 2, 64] - [L*directions,B,H]\n",
      "pre_output - dim: 3, size: [2, 1, 224] - [B,1,Et+H+2H]\n",
      "pre_output - dim: 3, size: [2, 1, 64] - [B,1,H]\n",
      "END forward_step ----------------------------------------\n",
      "BEGIN forward_step ----------------------------------------\n",
      "query - dim: 3, size: [2, 1, 64] - [B,1,H]\n",
      "BEGIN forward_attention ----------------------------------------\n",
      "query - dim: 3, size: [2, 1, 64] - [B,1,H]\n",
      "scores - dim: 3, size: [2, 9, 1] - [B,Ss,1]\n",
      "scores - dim: 3, size: [2, 1, 9] - [B,1,Ss]\n",
      "alphas - dim: 3, size: [2, 1, 9] - [B,1,Ss]\n",
      "context - dim: 3, size: [2, 1, 128] - [B,1,2H]\n",
      "END forward_attention ----------------------------------------\n",
      "decoder_input - dim: 3, size: [2, 1, 160] - [B,1,Et+2H]\n",
      "output - dim: 3, size: [2, 1, 64] - [B,1,H]\n",
      "dec_hidden - dim: 3, size: [1, 2, 64] - [L*directions,B,H]\n",
      "pre_output - dim: 3, size: [2, 1, 224] - [B,1,Et+H+2H]\n",
      "pre_output - dim: 3, size: [2, 1, 64] - [B,1,H]\n",
      "END forward_step ----------------------------------------\n",
      "BEGIN forward_step ----------------------------------------\n",
      "query - dim: 3, size: [2, 1, 64] - [B,1,H]\n",
      "BEGIN forward_attention ----------------------------------------\n",
      "query - dim: 3, size: [2, 1, 64] - [B,1,H]\n",
      "scores - dim: 3, size: [2, 9, 1] - [B,Ss,1]\n",
      "scores - dim: 3, size: [2, 1, 9] - [B,1,Ss]\n",
      "alphas - dim: 3, size: [2, 1, 9] - [B,1,Ss]\n",
      "context - dim: 3, size: [2, 1, 128] - [B,1,2H]\n",
      "END forward_attention ----------------------------------------\n",
      "decoder_input - dim: 3, size: [2, 1, 160] - [B,1,Et+2H]\n",
      "output - dim: 3, size: [2, 1, 64] - [B,1,H]\n",
      "dec_hidden - dim: 3, size: [1, 2, 64] - [L*directions,B,H]\n",
      "pre_output - dim: 3, size: [2, 1, 224] - [B,1,Et+H+2H]\n",
      "pre_output - dim: 3, size: [2, 1, 64] - [B,1,H]\n",
      "END forward_step ----------------------------------------\n",
      "END forward_decode ----------------------------------------\n",
      "decoder_states - dim: 3, size: [2, 9, 64] - [B,St,H]\n",
      "pre_output_vectors - dim: 3, size: [2, 9, 64] - [B,St,H]\n",
      "END forward  ----------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[[-0.1529, -0.0811,  0.1074,  ...,  0.0793,  0.3190,  0.0340],\n",
       "          [-0.3543, -0.1186,  0.1844,  ...,  0.2085,  0.3550, -0.0168],\n",
       "          [-0.2615, -0.2348,  0.0572,  ...,  0.0496,  0.2315,  0.1218],\n",
       "          ...,\n",
       "          [-0.1234, -0.0249,  0.2291,  ..., -0.1133, -0.0600,  0.4446],\n",
       "          [-0.2222,  0.1661,  0.2187,  ...,  0.1482, -0.0964,  0.3330],\n",
       "          [-0.1887, -0.0385,  0.0562,  ...,  0.0744,  0.0092,  0.3760]],\n",
       " \n",
       "         [[-0.3168, -0.1884,  0.0259,  ...,  0.0766,  0.1945,  0.0567],\n",
       "          [-0.2695, -0.2120, -0.1413,  ...,  0.0440,  0.1292,  0.2259],\n",
       "          [-0.2390, -0.2252, -0.1676,  ..., -0.0178,  0.1296,  0.2435],\n",
       "          ...,\n",
       "          [ 0.0233, -0.1178,  0.1760,  ..., -0.1049, -0.3577,  0.1630],\n",
       "          [-0.0731, -0.1030, -0.0091,  ..., -0.0543, -0.1106,  0.2326],\n",
       "          [-0.3572, -0.3602,  0.3161,  ...,  0.0542, -0.1709, -0.0611]]],\n",
       "        grad_fn=<CatBackward>),\n",
       " tensor([[[-0.1887, -0.0385,  0.0562,  0.4730, -0.0369,  0.6626,  0.5417,\n",
       "           -0.3438, -0.3696, -0.0804,  0.0902,  0.2453,  0.2441,  0.1841,\n",
       "           -0.3437, -0.0619, -0.1198,  0.0449,  0.2526, -0.3430, -0.2691,\n",
       "           -0.0770, -0.1046,  0.2248,  0.4866, -0.5966,  0.2803, -0.0118,\n",
       "           -0.1531,  0.2220,  0.0543,  0.5213, -0.3032,  0.3292,  0.1145,\n",
       "           -0.1499, -0.0266,  0.2941, -0.0183, -0.4477,  0.3491,  0.1146,\n",
       "            0.2410, -0.1654, -0.2165, -0.4482,  0.2813, -0.4724, -0.5342,\n",
       "            0.3449,  0.0163,  0.1408, -0.2769, -0.3390, -0.1605, -0.0566,\n",
       "           -0.1432,  0.2305, -0.1960, -0.0740,  0.0156,  0.0744,  0.0092,\n",
       "            0.3760],\n",
       "          [-0.3572, -0.3602,  0.3161,  0.1539, -0.1829,  0.3275,  0.1635,\n",
       "           -0.6645, -0.6005, -0.1728,  0.2107, -0.1605,  0.0783,  0.2552,\n",
       "           -0.0016, -0.1309, -0.3485,  0.0629,  0.0751, -0.2308,  0.0395,\n",
       "           -0.5439, -0.1129,  0.5213,  0.3873, -0.1460,  0.2685,  0.3094,\n",
       "           -0.5145, -0.0166,  0.0889,  0.5370, -0.5513,  0.4209,  0.0167,\n",
       "            0.0254, -0.3242,  0.1486,  0.1977, -0.4766,  0.2260, -0.0103,\n",
       "           -0.0508,  0.0301, -0.1638, -0.6026,  0.0855, -0.4297, -0.6775,\n",
       "           -0.2272, -0.0233,  0.2069,  0.0854, -0.1466, -0.0130, -0.1084,\n",
       "            0.0109,  0.3182, -0.3060,  0.1966,  0.1720,  0.0542, -0.1709,\n",
       "           -0.0611]]], grad_fn=<StackBackward>),\n",
       " tensor([[[ 0.0841, -0.1263, -0.2824,  ..., -0.4099, -0.1166,  0.3580],\n",
       "          [-0.0165, -0.4205, -0.1749,  ...,  0.1213, -0.3343, -0.1490],\n",
       "          [-0.3932,  0.2413,  0.4831,  ..., -0.6337, -0.6118,  0.2945],\n",
       "          ...,\n",
       "          [-0.1089, -0.0685,  0.0978,  ..., -0.1024, -0.2496,  0.4658],\n",
       "          [-0.2641, -0.2215, -0.2419,  ...,  0.1060, -0.4958,  0.1490],\n",
       "          [-0.5206,  0.1451,  0.3459,  ..., -0.5652, -0.6331,  0.1826]],\n",
       " \n",
       "         [[-0.0473, -0.1756, -0.1343,  ..., -0.4256, -0.1975,  0.2511],\n",
       "          [-0.5721, -0.0311,  0.3762,  ..., -0.3839, -0.6357,  0.1972],\n",
       "          [-0.5285,  0.0914,  0.2942,  ..., -0.5556, -0.5128,  0.1474],\n",
       "          ...,\n",
       "          [-0.0506, -0.3860, -0.2465,  ..., -0.0317,  0.0938, -0.1856],\n",
       "          [-0.5910, -0.1353,  0.3178,  ..., -0.5938, -0.4474,  0.0800],\n",
       "          [-0.3424, -0.0900,  0.0033,  ..., -0.2810, -0.2397,  0.2313]]],\n",
       "        grad_fn=<CatBackward>))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set_print(True)\n",
    "b = data[0]\n",
    "model.forward(b.src, b.trg, b.src_mask, b.trg_mask, b.src_lengths, b.trg_lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
